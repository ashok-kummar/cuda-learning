# ðŸš€ CUDA Learning Journey & Roadmap

Welcome to my GPU learning journey!  
This repository documents my path from CUDA fundamentals to optimizing AI kernels for production-level performance.

---

## ðŸ“˜ Primary Learning Resource

**Book:**  
> *Programming Massively Parallel Processors: A Hands-on Approach*  
> Authors: David B. Kirk, Wen-mei W. Hwu

---

## ðŸ§­ Learning Phases

### ðŸŸ¢ Phase 1: Fundamentals
> Goal: Learn the CUDA execution model and write basic kernels.

- [ ] Set up CUDA dev environment
- [ ] Learn thread/block/grid hierarchy
- [ ] Write first CUDA program (vector addition)
- [ ] Experiment with block sizes
- [ ] Understand memory hierarchy (global, shared, registers)
- [ ] Build CMake-based template

### ðŸ”µ Phase 2: Intermediate Kernels
> Goal: Write more complex and optimized kernels using shared memory.

- [ ] Matrix addition and multiplication
- [ ] Use of shared memory (tiling)
- [ ] Learn about bank conflicts and coalesced access
- [ ] Reduction (sum) using tree method
- [ ] Warp-level intrinsics (shuffle)
- [ ] Prefix sum (scan)

### ðŸŸ£ Phase 3: Performance Profiling
> Goal: Profile and optimize your kernels.

- [ ] Learn to use Nsight Compute
- [ ] Interpret key metrics: occupancy, memory throughput, warp efficiency
- [ ] Optimize memory usage (shared, register, global)
- [ ] Time kernels using `cudaEvent_t`
- [ ] Measure speedups vs CPU baseline

### ðŸ”¶ Phase 4: Advanced AI Ops
> Goal: Build and optimize real AI operations in CUDA.

- [ ] Softmax, LayerNorm, GELU kernels
- [ ] Fuse multiple ops into one kernel
- [ ] Mixed precision (`float16` / Tensor Cores optional)
- [ ] Workload balancing (row-wise, warp-wise)
- [ ] Use of streams for overlapping compute and memcpy

### ðŸŸ¥ Phase 5: Systems Integration
> Goal: Build pipelines that combine multiple CUDA ops.

- [ ] Use `cudaStream_t` for concurrency
- [ ] Profile full app with Nsight Systems
- [ ] Measure end-to-end latency
- [ ] Build benchmarking framework
- [ ] Prepare real-time AI op use cases (e.g. inference block)

---

## ðŸ“˜ CUDA Mini-Projects Timeline

This section breaks down the CUDA mini-projects aligned with chapters from the book  
**"Programming Massively Parallel Processors: A Hands-on Approach"**  
Each mini-project builds core GPU programming skills for AI kernel optimization.

---

### ðŸ“— Chapters 1â€“4: Basic CUDA Programming

**Concepts:**
- Thread hierarchy (thread, block, grid)
- Kernel launching
- Host vs Device memory

**Mini-Projects:**
- âœ… **Vector Addition** â€“ Naive version (each thread adds one element)
- âœ… **2D Matrix Addition** â€“ Threads organized in 2D grid
- âœ… **Vector Scaling** â€“ Multiply vector elements by a scalar

---

### ðŸ“˜ Chapter 5: Memory and Data Locality

**Concepts:**
- Global vs Shared memory
- Memory coalescing

**Mini-Projects:**
- âœ… **Shared Memory Matrix Add** â€“ Rewrite matrix add using shared memory tiling
- âœ… **Global Memory Coalescing Checker** â€“ Write one version that breaks coalescing and compare with an optimized one using Nsight Compute

---

### ðŸ“™ Chapter 6: Parallel Patterns â€“ Reduction

**Concepts:**
- Reduction (sum, max, min)

**Mini-Projects:**
- âœ… **Sum Reduction Kernel** â€“ Parallel reduction using grid-stride loops
- âœ… **Max Reduction Kernel** â€“ Find maximum value using warp shuffles (optional advanced version)

---

### ðŸ“’ Chapter 7: Parallel Scan (Prefix Sum)

**Concepts:**
- Inclusive vs Exclusive scan
- Work-efficient vs Naive scan

**Mini-Projects:**
- âœ… **Prefix Sum (Naive)** â€“ Simple parallel scan using shared memory
- âœ… **Prefix Sum (Work-Efficient)** â€“ Use Blelloch scan or similar work-efficient strategy

---

### ðŸ“• Chapters 8â€“9: Tiling and Memory Optimization

**Concepts:**
- Matrix tiling for GEMM
- Shared memory optimizations

**Mini-Projects:**
- âœ… **Naive GEMM (Matrix Multiply)** â€“ Row-by-col dot-product (no shared memory)
- âœ… **Tiled GEMM with Shared Memory** â€“ Use tiling to load shared blocks and improve locality
- âœ… **Compare Naive vs Tiled performance** â€“ Use Nsight Compute metrics

---

### ðŸ““ Chapter 10: Additional Patterns

**Concepts:**
- Histograms
- Sparse matrix ops

**Mini-Projects:**
- âœ… **Histogram Kernel** â€“ Bin values into histogram (use atomics carefully)
- âœ… **Sparse Matrix-Vector Multiply (SpMV)** â€“ Bonus: Practice memory access efficiency

---

### ðŸ“” Chapters 11+: Advanced Topics

**Concepts:**
- Streams and overlapping kernels
- Occupancy optimization
- Warp-level primitives

**Mini-Projects:**
- âœ… **Multi-Stream Vector Add** â€“ Use streams to overlap compute and memory copies
- âœ… **Warp-Level Reduction** â€“ Use warp shuffles (`__shfl_xor_sync`) for faster reductions

---

## ðŸ§  Bonus: AI-Specific CUDA Mini-Projects

These mirror real-world kernels used in AI/ML frameworks like PyTorch and TensorFlow.

**Mini-Projects:**
- âœ… **Softmax Kernel** â€“ Normalize logits in-place
- âœ… **LayerNorm Kernel** â€“ Normalize and scale inputs across feature dimensions


## ðŸ§ª Mini Projects

| # | Project | Status | Notes |
|--|---------|--------|-------|
| 1 | Vector Add (1D) | â˜ | First working kernel |
| 2 | Vector Add (2D grid) | â˜ | Practice grid indexing |
| 3 | Matrix Add | â˜ | Memory layout practice |
| 4 | Matrix Multiply (naive) | â˜ | Row-col inner product |
| 5 | Matrix Multiply (tiled) | â˜ | Shared memory optimization |
| 6 | Sum Reduction | â˜ | Tree + warp shuffle |
| 7 | Prefix Sum (Scan) | â˜ | Used in stream compaction |
| 8 | Histogram | â˜ | Atomics and privatization |
| 9 | LayerNorm | â˜ | AI kernel |
| 10 | Multi-stream Overlap | â˜ | Use `cudaStream_t` |

---

## ðŸ† CUDA Challenge List

| Challenge | Description | Done |
|----------|-------------|------|
| 1 | 1D/2D Vector Add + Grid indexing | â˜ |
| 2 | Try multiple block sizes | â˜ |
| 3 | Add error handling macro | â˜ |
| 4 | Rewrite Matrix Add with shared mem | â˜ |
| 5 | Memory coalescing vs strided | â˜ |
| 6 | Use `cudaOccupancyMaxPotentialBlockSize` | â˜ |
| 7 | Tree Reduction | â˜ |
| 8 | Warp Shuffle Reduction | â˜ |
| 9 | Prefix Scan | â˜ |
| 10 | Histogram with privatization | â˜ |
| 11 | Tiled Matrix Multiply | â˜ |
| 12 | Softmax kernel | â˜ |
| 13 | Fused LayerNorm | â˜ |
| 14 | Stream Overlap | â˜ |
| 15 | Benchmark CUDA vs CPU | â˜ |

---

## ðŸ“¦ Profiling Tools

### Nsight Compute (ncu)
> For analyzing individual CUDA kernels

- Metrics: occupancy, memory throughput, warp efficiency
- Command: `ncu ./your_binary`

### Nsight Systems (nsys)
> For analyzing full program timeline

- Measure: CPU-GPU overlaps, kernel launch delays, memcpy overlaps
- Command: `nsys profile ./your_binary`

---

## ðŸ“… Weekly Progress Log

| Week | Highlights |
|------|------------|
| Week 1 | Set up project, completed 1D vector add |
| Week 2 | _[to be filled]_ |
| Week 3 | _[to be filled]_ |

---
