# ðŸš€ CUDA Learning Plan & Progress Tracker

Welcome to my GPU learning journey! This README documents my roadmap, challenges, and progress as I learn CUDA for AI optimization and parallel programming.

---

## ðŸ“˜ Learning Resource

**Main Book:**  
> *Programming Massively Parallel Processors: A Hands-on Approach*  
> Authors: David B. Kirk, Wen-mei W. Hwu

---

## ðŸ§­ Roadmap

### âœ… Phase 1: Fundamentals
- [ ] Set up CUDA development environment
- [ ] Learn CUDA execution model (threads, blocks, grids)
- [ ] Implement vector addition in CUDA
- [ ] Explore different `threadsPerBlock` sizes
- [ ] Learn memory hierarchy (global, shared, registers)
- [ ] Write clean project structure with CMake

### ðŸ”„ Weekly Progress Updates
I'll update this section weekly with notes and completed challenges.

---

## ðŸ§ª Mini Projects

| # | Project | Status | Notes |
|--|---------|--------|-------|
| 1 | Vector Add (1D) | â˜ | First working CUDA kernel |
| 2 | Vector Add (2D grid) | â˜ | Practice grid indexing |
| 3 | Matrix Add | â˜ | Practice memory layout |
| 4 | Matrix Multiplication (naive) | â˜ | |
| 5 | Matrix Multiplication (tiled/shared memory) | â˜ | Optimize for performance |
| 6 | Reduction (sum) | â˜ | Tree-based and warp-shuffle |
| 7 | Prefix Sum (scan) | â˜ | Use for stream compaction |
| 8 | Histogram | â˜ | Practice atomics |
| 9 | LayerNorm | â˜ | Mini AI-op kernel |
| 10 | Multi-stream overlaps | â˜ | Use `cudaStream_t` |

---

## ðŸ† CUDA Challenge List

| Challenge | Description | Done |
|----------|-------------|------|
| 1 | 1D & 2D Vector Add + Grid indexing | â˜ |
| 2 | Experiment with different block sizes | â˜ |
| 3 | Error handling macro for all CUDA calls | â˜ |
| 4 | Rewrite matrix add with shared memory | â˜ |
| 5 | Memory coalescing vs strided access | â˜ |
| 6 | Use `cudaOccupancyMaxPotentialBlockSize` | â˜ |
| 7 | Tree-based sum reduction | â˜ |
| 8 | Warp-level shuffle-based reduction | â˜ |
| 9 | Stream compaction with prefix sum | â˜ |
| 10 | Naive matrix multiplication | â˜ |
| 11 | Tiled matrix mult (shared memory) | â˜ |
| 12 | Compare tile sizes: 8x8, 16x16, 32x32 | â˜ |
| 13 | Histogram optimization (privatization) | â˜ |
| 14 | Overlapping memcopy + compute (streams) | â˜ |
| 15 | Softmax kernel | â˜ |
| 16 | Fused LayerNorm (mean/var/scale) | â˜ |
| 17 | Memory optimization of AI op | â˜ |

---

## ðŸ”§ Tools & Profiling

### ðŸ”¹ Profiler

- **Start with:** `Nsight Compute (ncu)`
- **Later:** `Nsight Systems (nsys)` for full-app timeline

> Nsight Compute is ideal when optimizing a single kernel  
> Nsight Systems helps when analyzing full workloads, memory copy overlaps, and multiple streams.

### ðŸ”¹ Planned Guides

- [ ] Nsight Compute Starter Guide âœ… (queued)
- [ ] Advanced Template with Timing, Testing, CMake, Profiling (coming soon)

---

## ðŸ” Next Steps

- [ ] Complete 2â€“3 mini-projects
- [ ] Switch to advanced CUDA project template
- [ ] Start performance profiling using Nsight Compute
- [ ] Begin automated unit testing of kernel outputs

---

## ðŸ¥‡ Badge Milestones

| Badge | Requirement | Earned |
|-------|-------------|--------|
| CUDA Beginner | Complete 5 beginner challenges | â˜ |
| Memory Master | Optimize shared/coalesced memory kernels | â˜ |
| Warp Wizard | Use warp intrinsics (shuffle, ballots) | â˜ |
| Performance Profiler | Profile and improve kernel runtime | â˜ |
| AI Kernel Pro | Implement and optimize AI-op kernel | â˜ |

---

## ðŸ“… Weekly Progress Log

| Week | Highlights |
|------|------------|
| Week 1 | Started vector add, set up CMake, began Challenge 1 |
| Week 2 | _[to be filled]_ |
| Week 3 | _[to be filled]_ |

---

## ðŸ’¬ Notes & Learnings

> â€œOptimization without measurement is just guesswork.â€  
> Profiling is as important as coding â€” always measure performance and verify correctness!

---

## ðŸ™Œ Credits

This roadmap was designed with guidance from an AI mentor powered by ChatGPT.  
Structured like a real-world GPU engineering journey ðŸš€

---

